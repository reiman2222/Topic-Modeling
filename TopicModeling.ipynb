{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Topic models attempt to infer the hidden topic strucure of a document or corpus by analizing the words that make up a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Run code block 1 on first use of this notebook**<br>\n",
    "<br>\n",
    "**Remember to run code blocks 3 before attempting to run other portions of this notebook**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up / Instructions\n",
    "This code is written for python3\n",
    "\n",
    "\n",
    "**Install the gensim package in the same place as anaconda**\n",
    "\n",
    "the following command should work\n",
    "\n",
    "pip install gensim\n",
    "\n",
    "**To change the corpus:**<br>\n",
    "go to code block **4** and change the current path 'corpusIR' to the path to the desired corpus. This program expects a corpus to be a folder contaning text files. each individual text file is treated as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE BLOCK 1\n",
    "\n",
    "#run this only on first use of this notebook\n",
    "#this downloads data needed for the nltk lemmatizer and pos tagger\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE BLOCK 2\n",
    "\n",
    "# Run this code block if you want to see gensim log output.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE BLOCK 3\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "from pprint import pprint #pretty printer\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "from decimal import Decimal\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "#This variable controls the intensity of the text preprocessing.\n",
    "#Its valid values are the inergers 1, 2 and 3.\n",
    "#The lower the less intense the preprocessing.\n",
    "#A value of 2 should be sufficent for training topic models.\n",
    "textPreprocessingLevel = 2\n",
    "\n",
    "#Define MyCorpus object\n",
    "class MyCorpus(object):\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def createCorpus(docs):\n",
    "        corp = MyCorpus()\n",
    "        corp.docL = docs\n",
    "        return corp\n",
    "\n",
    "    def __init__(self):\n",
    "        docL = []\n",
    "    def __len__(self):\n",
    "        return len(self.docL)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizeDocLight(doc):\n",
    "        docT = []\n",
    "        for word in doc.lower().split():\n",
    "            w = word.strip(string.punctuation)\n",
    "            docT.append(w)\n",
    "        \n",
    "        return docT\n",
    "    \n",
    "\n",
    "    #returns list of tokens in lowercase split on whitespace with punctuation removed, lemmatized\n",
    "    @staticmethod\n",
    "    def tokenizeDocMedium(doc):\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        docT = []\n",
    "        for word in doc.lower().split():\n",
    "            w = word.strip(string.punctuation)\n",
    "            w = wordnet_lemmatizer.lemmatize(w)\n",
    "            if(w != ''):\n",
    "                docT.append(w)\n",
    "\n",
    "        return docT\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizeDocHeavy(doc):\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        docT = []\n",
    "        wordLSplit = doc.lower().split()\n",
    "        wordListPunctuationRemoved = []\n",
    "        for word in wordLSplit:\n",
    "            w = word.strip(string.punctuation)\n",
    "            if(w != ''):\n",
    "                wordListPunctuationRemoved.append(w)\n",
    "\n",
    "        wordLTagged = pos_tag(wordListPunctuationRemoved)\n",
    "        for wT in wordLTagged:\n",
    "            if('NN' in wT[1]):\n",
    "                docT.append(wordnet_lemmatizer.lemmatize(wT[0], pos='n'))\n",
    "            elif('VB' in wT[1]):\n",
    "                docT.append(wordnet_lemmatizer.lemmatize(wT[0], pos='v'))\n",
    "\n",
    "        return docT\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if(textPreprocessingLevel == 1):\n",
    "            tokenizeDoc = MyCorpus.tokenizeDocLight\n",
    "        elif(textPreprocessingLevel == 2):\n",
    "            tokenizeDoc = MyCorpus.tokenizeDocMedium\n",
    "        elif(textPreprocessingLevel == 3):\n",
    "            tokenizeDoc = MyCorpus.tokenizeDocHeavy\n",
    "        else:\n",
    "            print('WARNING: text preprocessing level not recognized, using medium level text preprocessing.')\n",
    "            tokenizeDoc = tokenizeDocMedium\n",
    "        \n",
    "        for doc in self.docL:\n",
    "            yield dictionary.doc2bow(tokenizeDoc(doc)) \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "#------------------------------------------------------------#\n",
    "# Functions for dictionary construction / text preprocessing #\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#getStopWordList(pathToSWL) returns a list of stopwords contained in file\n",
    "#located at pathToSWL. \n",
    "#getStopWordList EXPECTS the stop words in the text file to be deliniated by whitespace\n",
    "def getStopWordList(pathToSWL):\n",
    "    #print('Reading stopword list from: ' + pathToSWL + '\\n')\n",
    "\n",
    "    f = open(pathToSWL, 'r')\n",
    "    fcontents = f.read()\n",
    "    stopWordList = fcontents.split()\n",
    "    f.close()\n",
    "\n",
    "    return stopWordList\n",
    "\n",
    "def tokenizeCorpus(corpusList, pathToStopWordList):\n",
    "    tokenizedCorpus = []\n",
    "    \n",
    "    #preprecessing level 1\n",
    "    if(textPreprocessingLevel == 1):\n",
    "        for doc in corpusList:\n",
    "             tokenizedCorpus.append(tokenizeDocumentLight(doc))\n",
    "\n",
    "    else:\n",
    "        #read stop list, convert to set for faster stop word removal\n",
    "        stoplist = getStopWordList(pathToStopWordList)\n",
    "        stopSet = set(stoplist)\n",
    "        \n",
    "        #initalize the word net lemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        #compile regular expresion to remove numbers\n",
    "        numberRemover = re.compile(\"[\\d{}]+$\".format(re.escape(string.punctuation)))\n",
    "        \n",
    "        #preprecessing level 2\n",
    "        if(textPreprocessingLevel == 2):\n",
    "            tokenizeDocumentWithSWR = tokenizeDocumentMedium\n",
    "        \n",
    "        #preprecessing level 3\n",
    "        elif(textPreprocessingLevel == 3):\n",
    "            tokenizeDocumentWithSWR = tokenizeDocumentHeavy\n",
    "        else:\n",
    "            print('WARNING: text preprocessing level not recognized, using medium level text preprocessing.')\n",
    "            tokenizeDocument = tokenizeDocMedium\n",
    "        \n",
    "        for doc in corpusList:\n",
    "            tokenizedCorpus.append(tokenizeDocumentWithSWR(doc, stopSet, lemmatizer, numberRemover))\n",
    "    \n",
    "    return tokenizedCorpus\n",
    "            \n",
    "def tokenizeDocumentLight(doc):\n",
    "    docTokenized = []\n",
    "    for word in doc.lower().split():\n",
    "        w = word.strip(string.punctuation) #remove trailing punctuation\n",
    "        if(w != ''):\n",
    "            docTokenized.append(w)\n",
    "    return docTokenized\n",
    "\n",
    "def tokenizeDocumentMedium(doc, stopSet, lemmatizer, numberRemover):\n",
    "    docTokenized = []\n",
    "    for word in doc.lower().split():\n",
    "        w = word.strip(string.punctuation)\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "        if(w not in stopSet and w != '' and not numberRemover.match(w)):\n",
    "            docTokenized.append(w)\n",
    "            \n",
    "    return docTokenized\n",
    "    \n",
    "def tokenizeDocumentHeavy(doc, stopSet, lemmatizer, numberRemover):\n",
    "    docTokenized = []\n",
    "    tokensUntagged = []\n",
    "    for word in doc.lower().split():\n",
    "        w = word.strip(string.punctuation)\n",
    "        if(w not in stopSet and w != '' and not numberRemover.match(w)):\n",
    "            tokensUntagged.append(w)\n",
    "    \n",
    "    #tokensTagged is a list of tupes of the form (token as a string, part of speech tag as a string)\n",
    "    #see nltk's documentation for more information\n",
    "    tokensTagged = pos_tag(tokensUntagged)\n",
    "    for wordTagged in tokensTagged:\n",
    "        #keep words tagged as nouns or verbs\n",
    "        if('NN' in wordTagged[1]):\n",
    "            w = lemmatizer.lemmatize(wordTagged[0], pos='n')\n",
    "        elif('VB' in wordTagged[1]):\n",
    "            w = lemmatizer.lemmatize(wordTagged[0], pos='v')\n",
    "        else:\n",
    "            w = ''\n",
    "            \n",
    "        if(w != '' and w not in stopSet):\n",
    "            docTokenized.append(w)\n",
    "    \n",
    "    return docTokenized\n",
    "\n",
    "\n",
    "#------------------#\n",
    "# Helper Functions #\n",
    "#------------------#\n",
    "\n",
    "\n",
    "#corpusSourceToListOfDocuments returns all the content of all documents in folder path\n",
    "#as a list of strings.\n",
    "#path is the path to the folder containing the corpus\n",
    "def corpusSourceToListOfDocumets(path):\n",
    "    corpusL = []\n",
    "    for filename in os.listdir(path):\n",
    "        fullpath = path + '/' + filename\n",
    "        if(filename[0] != '.'):\n",
    "            f = open(fullpath, 'r')\n",
    "            fcontents = f.read()\n",
    "            corpusL.append(fcontents)\n",
    "            f.close()\n",
    "\n",
    "    return corpusL\n",
    "\n",
    "def showTopicModelAsTable(tm, numTopics, numKeywords, tableName):\n",
    "    topicList = []\n",
    "    for i in range(0, numTopics):\n",
    "        row = []\n",
    "        row.append(i)\n",
    "\n",
    "        keyWordList = tm.show_topic(i, topn=numKeywords)\n",
    "        \n",
    "        #get just the keywords\n",
    "        for k in keyWordList:\n",
    "            row.append(k[0])\n",
    "\n",
    "        topicList.append(row)\n",
    "\n",
    "    #build list of column names\n",
    "    colLabels = ['Topic Number']\n",
    "    i = 0\n",
    "    while(i < (len(topicList[0]) - 1)):\n",
    "        colLabels.append('Keyword ' + str(i+ 1))\n",
    "        i += 1\n",
    "   \n",
    "    #build list of column widths\n",
    "    widths = [0.18]\n",
    "    i = 0\n",
    "    while(i < (len(topicList[0]) - 1)):\n",
    "        widths.append(0.17)\n",
    "        i += 1\n",
    "    \n",
    "    #draw table\n",
    "  \n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    #fig.patch.set_visible(False)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    tbl = ax.table(cellText=topicList, colLabels=colLabels, cellLoc='center', loc='lower left', colWidths=widths)\n",
    "\n",
    "    tbl.auto_set_font_size(False)\n",
    "\n",
    "    tbl.set_fontsize(40)\n",
    "    tbl.scale(3,6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    tmTableToCSV('./figures/' + tableName + '.csv', colLabels, topicList)\n",
    "    \n",
    "def tmTableToCSV(fname, colLabels, tableContents):\n",
    "    with open(fname, 'w+') as f:\n",
    "        i = 0\n",
    "        colLablesJoinend = '\",\"'.join(colLabels)\n",
    "        colLablesStr = '\"' + colLablesJoinend + '\"\\n'\n",
    "        f.write(colLablesStr)\n",
    "        for row in tableContents:\n",
    "            rowStr = '\"' + '\",\"'.join(map(str, row)) + '\"\\n'\n",
    "            f.write(rowStr)\n",
    "            \n",
    "def graphToCSV(axisLabels, x, y, fname):\n",
    "    with open(fname, 'w+') as f:\n",
    "        f.write('\"' + '\",\"'.join(axisLabels) + '\"\\n')\n",
    "        \n",
    "        i = 0\n",
    "        while(i < len(x)):\n",
    "            f.write('\"' + str(x[i]) + '\",\"' + str(y[i]) + '\"\\n')\n",
    "            i += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and Process Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE BLOCK 4\n",
    "\n",
    "#read corpus into memory\n",
    "#a corpus is a list of strings where each string is viewed as a document\n",
    "\n",
    "#change 'corpusIR' to a different path to read in a different corpus\n",
    "#the path should be the path to the folder containing the corpus of text documents\n",
    "\n",
    "corpusList = corpusSourceToListOfDocumets('corpusIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning a Basic Topic Model\n",
    "\n",
    "This code trains a basic LDA model for numTopics topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizedCorpus is a list of lists of strings\n",
    "#each list of strings corresponds to the tokens that maked up the document in corpusList \n",
    "tokenizedCorpus = tokenizeCorpus(corpusList, 'stopwords.txt')\n",
    "\n",
    "#creates the dictionary for the topic model\n",
    "#a dictionary is the collection of all words that the model should consider possible indicators of a topic\n",
    "dictionary = corpora.Dictionary(tokenizedCorpus)\n",
    "\n",
    "#creates a corpus object for gensim\n",
    "theCorpus = MyCorpus.createCorpus(corpusList)\n",
    "\n",
    "#build tfidf model of theCorpus\n",
    "tfidf = models.TfidfModel(theCorpus)\n",
    "\n",
    "#convert corpus from bag of words to tfidf vectors\n",
    "corpus_tfidf = tfidf[theCorpus]\n",
    "\n",
    "numTopics = 15\n",
    "lda = models.LdaModel(theCorpus, id2word=dictionary, num_topics=numTopics, passes=7, update_every=2, iterations=300)\n",
    "\n",
    "pprint(lda.print_topics(num_topics=numTopics, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Topic Modeling Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "<br>\n",
    "**Assumptions of LDA**\n",
    "- A document can express multiple topics in different proportions\n",
    "- The corpus is comprised of a fixed number of topics, K\n",
    "- Topics are hidden variables\n",
    "- Documents are \"bags of words\", meaning the order of words in the text is not considered\n",
    "\n",
    "**Mathematical Description of LDA**\n",
    "\n",
    "$$ p(\\beta_{1:K},\\theta_{1:D},z_{1:D},w_{1:D}) = \\prod_{i=1}^K p(\\beta_i) \\times \\prod_{d=1}^D p(\\theta_d) \\times \\big(\\prod_{n=1}^N p(z_{d,n}\\mid\\theta_d)\\times p(w_{d,n}\\mid\\beta_{1:K},z_{d,n})\\big) \\quad(1)$$\n",
    "<br>\n",
    "$\\beta_{k}$ \\- is a distribution over the vocabulary for the $k$th topic<br>\n",
    "$\\theta_d$ \\- is the topic proportions for the $d$th document <br>\n",
    "$\\theta_{d,k}$ \\- is the topic proportion of the $k$th topic in document $d$ <br>\n",
    "$z_d$ \\- is the topic assignments for the $d$th document <br>\n",
    "$z_{d,n}$ \\- is the topic assignment of the $n$th word in document $d$ <br>\n",
    "$w_d$ \\- are the observed words for document $d$, all words in $w_d$ are members of the fixed vocabulary <br>\n",
    "$w_{d,n}$ \\- is the $n$th word in document $d$ <br>\n",
    "<br>\n",
    "The Equation below is the posterior for LDA:<br>\n",
    "$$p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D} \\mid w_{1:D}) = \\frac{p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D}, w_{1:D})}{p(w_{1:D})} \\quad (2)$$\n",
    "\n",
    "The denominator of equation 2 is effectively incomputable for a corpus of any reasonable size since it involves assessing all possible assignments of all words to all possible topics. Gibbs sampling is one method used to approximate  $p(w_{1:D})$.\n",
    "\n",
    "<br>\n",
    "\n",
    "http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf [1]<br>\n",
    "http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing (LSI)\n",
    "Also referred to as Latent Semantic Analysis (LSA). LSI was created as an information retrieval tool with the goal of better handling synonymy and polysemy.\n",
    "\n",
    "**Assumptions of LSI**\n",
    "- bag of words\n",
    "\n",
    "<br>\n",
    "**Mathematical Description of LSI**<br>\n",
    "A term by document matrix $M$ is constructed from the corpus, with terms as the rows and documents as the columns.\n",
    "<br>$M_{t,d}$ holds the term frequency for term $t$ in document $d$\n",
    "\n",
    "\n",
    "Singular Value Decomposition (SVD) is performed on $M$ <br>\n",
    "SVD is defined in this context as: <br>\n",
    "$$M = T_0\\times S_0 \\times D_0'$$<br>\n",
    "$T_0$ \\- is a $t$ by $m$ matrix that has orthogonal unit-length columns, where $T_0 ' T_0 = I$<br>\n",
    "$D_0$ \\- is a $m$ by $d$ matrix that has orthogonal unit-length columns, where $D_0 ' D_0 = I$<br>\n",
    "$S_0$ \\- is a $m$ by $m$ diagonal matrix of singular values<br>\n",
    "$t$ \\- is the number of row of $M$, the number of terms<br>\n",
    "$d$ \\- is the number of columns in $M$, the number of documents<br>\n",
    "$m$ \\- is the rank of $M$, where $m \\leq min(t, d)$ <br>\n",
    "\n",
    "In practice the resulting matrices of the SVD of matrix $M$ are approximated (to reduce computations) by reduced SVD:\n",
    "$$X \\approx \\hat{X} = T \\times S \\times D'$$\n",
    "The notation for this formula is similar to equation 2, the changes are noted below<br>\n",
    "$T$ \\- is a $t$ by $k$ matrix <br>\n",
    "$D$ \\- is a $k$ by $d$ matrix <br>\n",
    "$S$ \\- is a $k$ by $k$ matrix<br>\n",
    "$k$ \\- is the rank of $S$, where $k \\leq m$ <br>\n",
    "<br>This approach serves as a sort of dimensionality reduction\n",
    "\n",
    "Document Similarities and word similarities can be calculated by doing operations on the matrices generated from the reduced SVD. <br>\n",
    "\n",
    "The similarity of two terms can be computed by taking the dot product of a row $i$ and a row $j$ of matrix $TS$<br>\n",
    "The similarity of two documents can be computed by taking the dot product of a row $i$ and a row $j$ of matrix $DS$<br>\n",
    "<br>\n",
    "It should be noted that there are also probabilistic versions of LSI [5]\n",
    "\n",
    "(see section 4.2 of Indexing by Latent Semantic Analysis, Derwester et. al. for an in-depth explanation) <br>\n",
    "\n",
    "http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf [3] (equations taken from this source)<br>\n",
    "http://173.236.226.255/tom/papers/SteyversGriffiths.pdf [4]<br>\n",
    "http://www.iro.umontreal.ca/~nie/IFT6255/Hofmann-UAI99.pdf [5] (covers pobabaistic LSI) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Dirichlet Process (HDP)\n",
    "\n",
    "A Nonparametric approach to Topic Modeling. Meaning the number of topics does not need to be specified in advance. HDP infers the number of topics from the corpus.\n",
    "\n",
    "**Mathematical Description of HDP**<br>\n",
    "At a high level a HDP is defined as a collection of Dirichlet processes where:\n",
    "$$G_0 \\sim DP(\\gamma H)$$<br>\n",
    "$$G_j \\sim DP(\\alpha_0 G_0), \\text{ for each }j$$\n",
    "<br>\n",
    "$G_0$ \\- is the base distribution, drawn from a Dirichlet process<br>\n",
    "$j$ \\- is an index for each group of data\n",
    "<br>\n",
    "To generate topics:<br>\n",
    "\n",
    "$$\\theta_{jn} \\sim G_j$$<br>\n",
    "where $\\theta_{jn}$ is the topic asiciated with the $n$th word in the $j$th document\n",
    "$$w_{jn} \\sim \\text{Mult}(\\theta_{jn})$$ <br>\n",
    "where $w_{jn}$ is the word from topic $\\theta_{jn}$\n",
    "\n",
    "<br>\n",
    "http://proceedings.mlr.press/v15/wang11a/wang11a.pdf [6] (where equations were drawn from)<br>\n",
    "Other useful refrence:<br>\n",
    "https://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302#.Wyv6w4InZUM (not used)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Topic Model Evaluation\n",
    "There a several ways to measure the effectiveness of topic models the two main metrics are perplexity and coherence. Other approaches like measuring a topic models performance on other task like information retrieval or document classification are also used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "Perplexity assesses the extent to which the model is confused by a set of held out documents belonging to the same corpus. The lower the perplexity the better. To evaluate a model using perplexity a subset of the documents in the corpus should be randomly selected and withheld as a test set. The performance of the model can then be evaluated on the unseen test set. \n",
    "\n",
    "**Defined for LDA as:**<br>\n",
    "$$perplexity(D_{test}) = exp \\bigg(-\\frac{\\sum^D_{d=1}\\log p(w_d)}{\\sum^D_{d=1} N_d} \\bigg)$$\n",
    "\n",
    "$D_{test}$ \\- is the collection of held out documents<br>\n",
    "$D$ \\- is the number of documents in $D_{test}$<br>\n",
    "$N_d$ \\- is the number of words in document $d$<br>\n",
    "\n",
    "http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf [7] section 7.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Coherence\n",
    "Coherence servers as a measure of the human interpretability of a given topic model. the higher the coherence the better. There are several methods for computing coherence some are listed below.\n",
    "\n",
    "**UCI Coherence:**<br>\n",
    "$$C_{UCI} = \\frac{2}{N\\cdot(N-1)} \\sum^{N-1}_{i=1}\\sum^N_{j=i+1} PMI(w_i,w_j)$$\n",
    "$$PMI(w_i,w_j)=\\log \\frac{p(w_i,w_j) + \\epsilon}{p(w_i) \\cdot p(w_j)}$$\n",
    "\n",
    "$N$ \\- is the top number of terms from some topic $t$<br>\n",
    "$w_i$ \\- is the $i$th word from the top $N$ words of topic $t$<br>\n",
    "$\\epsilon$ \\- is a small constant to prevent taking the log of $0$, $0 < \\epsilon \\leq 1$<br>\n",
    "Word probabilities are calculated based on word co-occurrence counts.\n",
    "\n",
    "**UMass Coherence:**<br>\n",
    "$$C_{UMass} = \\frac{2}{N\\cdot(N-1)} \\sum^{N}_{i=2}\\sum^{i-1}_{j=1} \\log \\frac{p(w_i,w_j) + \\epsilon}{p(w_j)})$$\n",
    "The notation remains the same except word probabilities are estimated using document frequencies from the training documents. \n",
    "\n",
    "**CV Coherence:**<br>\n",
    "\"$C_V$ combines  the  indirect  cosine  measure  with  the\n",
    "NPMI and the boolean sliding window.\"<br>\n",
    "\n",
    "NOTE: I was not able to find an explicit definition of $C_V$, the qoute above is the best explination that the paper provides. The paper linked is where $C_V$ coherence is originally propsed.\n",
    "\n",
    "<br>http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf [8]\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Best Model\n",
    "### Text Preprocessing\n",
    "\n",
    "Text preprocessing is of great importance when training Topic Models. Preprocessing greatly affects the resulting model. The usefulness of the model can be greatly affected by the words that are included in the vocabulary, called a dictionary in gensim. Stop words like *the* and *of* provide little information about the subject of a document. When stop words are allowed in the model they overpower the more meaningful words since the occur so frequently. <br>\n",
    "\n",
    "The code below trains three LDA topic models each employing different a method for text preprocessing. See code blocks 3 and 4 for definitions. The code refers to these methods of text preprocessing as \"light\", \"medium\", and \"heavy\", the models are trained and presented in that order. This is not an exhaustive list of text preprocessing methods but is instead present to show the effect of bad preprocessing.<br>\n",
    "\n",
    "\n",
    "**Minimal Preprocessing** (light)<br>\n",
    "Documents are lowercased and split on whitespace. Punctuation is striped from the resulting tokens.<br> \n",
    "\n",
    "**Stop Word Removal** (medium)<br>\n",
    "All of the previous steps are taken with the addition of stop word removal using the Stanford stop list with minor additions. tokens consisting of only number characters are also removed. Tokens are then lemmatized with the word net lemmatizer.\n",
    "\n",
    "**Part of Speech Tagging** (heavy)<br>\n",
    "A unigram part of speech tagger is used. tokens tagged not tagged as nouns or verbs are removed. tokens are then lemmatized according to their part of speech tag, this leads to better lemmatization. This method is very computationally expensive and could be unreasonable for a very large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code for text preprocessing evaluation\n",
    "textPreprocessingLevel = 1\n",
    "numTopics = 10\n",
    "\n",
    "tableTitles = ['Minimal_Preprocessing','Stop_Word_Removal','Part_of_Speech_Tagging']\n",
    "\n",
    "while(textPreprocessingLevel <= 3):\n",
    "    tokenizedCorpus = tokenizeCorpus(corpusList, 'stopwords.txt')\n",
    "    dictionary = corpora.Dictionary(tokenizedCorpus)\n",
    "    \n",
    "    theCorpus = MyCorpus.createCorpus(corpusList)\n",
    "    \n",
    "    tfidf = models.TfidfModel(theCorpus)\n",
    "    corpus_tfidf = tfidf[theCorpus]\n",
    "    \n",
    "    lda = models.LdaModel(theCorpus, id2word=dictionary, num_topics=numTopics, passes=10, update_every=2, iterations=300)\n",
    "    print('\\t\\t\\t\\t\\t\\t' + tableTitles[textPreprocessingLevel - 1])\n",
    "    \n",
    "    #Tables produced by this code are written to: \n",
    "    #   \"./figures/Minimal_Preprocessing.csv\"\n",
    "    #   \"./figures/Stop_Word_Removal.csv\"\n",
    "    #   \"./figures/Part_of_Speech_Tagging.csv\"\n",
    "    #respectively as the loop executes\n",
    "    showTopicModelAsTable(lda, numTopics, 10, tableTitles[textPreprocessingLevel - 1])\n",
    "    \n",
    "    textPreprocessingLevel += 1\n",
    "    \n",
    "textPreprocessingLevel = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Choosing the Number of Topics (LDA and LSI)\n",
    "This code generates graphs comparing number of topics to model coherence. **Run the block below to initalize necesary functions for generating the graphs.** Then you can run the other blocks to see the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code for generating topic graphs\n",
    "\n",
    "def evaluateLSINumTopicsGraph(dictionary, corpus, texts, limit):\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    for num_topics in range(1, limit + 1):\n",
    "        print('\\n===========================     Training on ' + str(num_topics) + ' topics     ===========================\\n')\n",
    "        lm = models.LsiModel(corpus, id2word=dictionary, num_topics=num_topics, power_iters=150, extra_samples=5)\n",
    "        lm_list.append(lm)\n",
    "        topics = []\n",
    "        for topic_id, topic in lm.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "        cm = models.CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "\n",
    "    # Show graph\n",
    "    x = range(1, limit + 1)\n",
    "    plt.plot(list(x), c_v)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence\")\n",
    "    plt.show()\n",
    "    \n",
    "    graphToCSV([\"Number of Topics\", \"Coherence\"], x, c_v, './figures/LSI_NUM_Topic_Graph.csv')\n",
    "\n",
    "    return (lm_list, c_v)\n",
    "\n",
    "def evaluateLDANumTopicsGraph(dictionary, corpus, texts, limit):\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    for num_topics in range(1, limit + 1):\n",
    "        print('\\n===========================     Training on ' + str(num_topics) + ' topics     ===========================\\n')\n",
    "        lm = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,  passes=5, update_every=2, iterations=250, alpha='auto', eta='auto')\n",
    "        lm_list.append(lm)\n",
    "        cm = models.CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "\n",
    "    # Show graph\n",
    "    x = range(1, limit + 1)\n",
    "    plt.plot(list(x), c_v)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence\")\n",
    "    plt.show()\n",
    "    \n",
    "    graphToCSV([\"Number of Topics\", \"Coherence\"], x, c_v, './figures/LDA_NUM_Topic_Graph.csv')\n",
    "    \n",
    "    return (lm_list, c_v)\n",
    "\n",
    "#shows table of topNkeywords of best topNtopics for topic model tm, uses coherence to evaluate topic\n",
    "def showBestTopics(tm, texts, dictionary, windowSize, topNTopics, topNKeywords, tableName):\n",
    "    coherence_values = {}\n",
    "    for n, topic in tm.show_topics(num_topics=tm.num_topics, formatted=False):\n",
    "        topic = [word for word, _ in topic]\n",
    "        cm = models.CoherenceModel(topics=[topic], texts=texts, dictionary=dictionary, window_size=windowSize, coherence='c_v')\n",
    "        coherence_values[n] = cm.get_coherence()\n",
    "\n",
    "    coherence_values = sorted(coherence_values.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "    #bestTopicList is a list of lists of the form [topic number, topic coherence, keyword 1, keyword 2, ..., keyword n])\n",
    "    bestTopicList = []\n",
    "    for i in range(0, topNTopics):\n",
    "        #print('topic number: ' + str(coherence_values[i][0]) + ', topic coherence: ' + str(coherence_values[i][1]))\n",
    "        \n",
    "        L = []\n",
    "        L.append(coherence_values[i][0])\n",
    "        L.append(round(Decimal(coherence_values[i][1]), 3))\n",
    "\n",
    "        kwL = tm.show_topic(coherence_values[i][0], topn=topNKeywords)\n",
    "\n",
    "        keywordOnly = []\n",
    "        for k in kwL:\n",
    "            keywordOnly.append(k[0])\n",
    "\n",
    "        L.extend(keywordOnly)\n",
    "        bestTopicList.append(L)\n",
    "\n",
    "    #draw table\n",
    "    colLabels = ['Topic Number', 'Coherence']\n",
    "    i = 0\n",
    "    while(i < (len(bestTopicList[0]) - 2)):\n",
    "        colLabels.append('Keyword ' + str(i+ 1))\n",
    "        i += 1\n",
    "\n",
    "    widths = [0.18, 0.18]\n",
    "    i = 0\n",
    "    while(i < (len(bestTopicList[0]) - 2)):\n",
    "        widths.append(0.17)\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    #fig.patch.set_visible(False)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    tbl = ax.table(cellText=bestTopicList, colLabels=colLabels, cellLoc='center', loc='lower left', colWidths=widths)\n",
    "    tbl.auto_set_font_size(False)\n",
    "\n",
    "    tbl.set_fontsize(40)\n",
    "    tbl.scale(3,6)\n",
    "    \n",
    "    tmTableToCSV('./figures/' + tableName + '.csv', colLabels, bestTopicList)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#gets best tm from data generated in evaluatenNumTopicsGraph functions\n",
    "#returns the best model\n",
    "def getBestModelFromEvalBestTopic(modelListCoherenceTuple):\n",
    "    bestScore = (0, 0)\n",
    "    i = 1\n",
    "\n",
    "    for cv in modelListCoherenceTuple[1]:\n",
    "        if(cv > bestScore[1]):\n",
    "            bestScore = (i, cv)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print('The Best model has ' + str(bestScore[0]) + ' topics with ' + str(bestScore[1]) + ' coherence.')\n",
    "    return modelListCoherenceTuple[0][bestScore[0] - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose model with best number of topics LSI\n",
    "\n",
    "numTopicsToShow = 25\n",
    "\n",
    "textPreprocessingLevel = 2\n",
    "\n",
    "tokenizedCorpus = tokenizeCorpus(corpusList, 'stopwords.txt')\n",
    "dictionary = corpora.Dictionary(tokenizedCorpus)\n",
    "    \n",
    "theCorpus = MyCorpus.createCorpus(corpusList)\n",
    "    \n",
    "tfidf = models.TfidfModel(theCorpus)\n",
    "corpus_tfidf = tfidf[theCorpus]\n",
    "\n",
    "lm_list_cv = evaluateLSINumTopicsGraph(dictionary, corpus_tfidf, tokenizedCorpus, 75)\n",
    "bestModel = getBestModelFromEvalBestTopic(lm_list_cv)\n",
    "\n",
    "numTopics = bestModel.num_topics\n",
    "\n",
    "if(numTopics < numTopicsToShow):\n",
    "    numTopicsToShow = numTopics\n",
    "\n",
    "#Table produced by this code is written to \"./figures/Best_LSI.csv\"\n",
    "showBestTopics(bestModel, tokenizedCorpus, dictionary, 110, numTopicsToShow, 10, 'Best_LSI')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose model with best number of topics LDA\n",
    "\n",
    "numTopicsToShow = 25\n",
    "\n",
    "textPreprocessingLevel = 2\n",
    "\n",
    "tokenizedCorpus = tokenizeCorpus(corpusList, 'stopwords.txt')\n",
    "dictionary = corpora.Dictionary(tokenizedCorpus)\n",
    "    \n",
    "theCorpus = MyCorpus.createCorpus(corpusList)\n",
    "    \n",
    "tfidf = models.TfidfModel(theCorpus)\n",
    "corpus_tfidf = tfidf[theCorpus]\n",
    " \n",
    "lm_list_cv = evaluateLDANumTopicsGraph(dictionary, corpus_tfidf, tokenizedCorpus, 3)\n",
    "bestModel = getBestModelFromEvalBestTopic(lm_list_cv)\n",
    "\n",
    "numTopics = bestModel.num_topics\n",
    "\n",
    "if(numTopics < numTopicsToShow):\n",
    "    numTopicsToShow = numTopics\n",
    "\n",
    "#Table produced by this code is written to \"./figures/Best_LDA.csv\"\n",
    "showBestTopics(bestModel, tokenizedCorpus, dictionary, 110, numTopicsToShow, 10, 'Best_LDA')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "References are listed as they appear in the notebook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[1]\n",
    "@incollection{Blei-2009-topic-models,\n",
    "  title={Topic models},\n",
    "  author={Blei, David M and Lafferty, John D},\n",
    "  booktitle={Text Mining},\n",
    "  pages={101--124},\n",
    "  year={2009},\n",
    "  publisher={Chapman and Hall/CRC}\n",
    "}\n",
    "\n",
    "[2]\n",
    "@article{Blei-2012-probabilistic-topic-models,\n",
    "  author = {Blei, David M.},\n",
    "  title = {Probabilistic Topic Models},\n",
    "  journal = {Commun. ACM},\n",
    "  issue_date = {April 2012},\n",
    "  volume = {55},\n",
    "  number = {4},\n",
    "  month = apr,\n",
    "  year = {2012},\n",
    "  issn = {0001-0782},\n",
    "  pages = {77--84},\n",
    "  numpages = {8},\n",
    "  doi = {10.1145/2133806.2133826},\n",
    "  acmid = {2133826},\n",
    "  publisher = {ACM},\n",
    "  address = {New York, NY, USA},\n",
    "} \n",
    "\n",
    "[3]\n",
    "@ARTICLE{Deerwester-1990-indexing-by-latent-semantic-analysis,\n",
    "  author={Deerwester, S. and Dumais, S.T. and Furnas, G.W. and Landauer, T.K. and Harshman, R.},\n",
    "  title={Indexing by latent semantic analysis},\n",
    "  journal={Journal of the American Society for Information Science},\n",
    "  year={1990},\n",
    "  volume={41},\n",
    "  number={6},\n",
    "  pages={391-407},\n",
    "  doi={10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},\n",
    "  document_type={Article},\n",
    "  source={Scopus},\n",
    "}\n",
    "\n",
    "[4]\n",
    "@inbook{steyvers-2007-pobabalistic-topic-models,\n",
    "  author = {Steyvers, M. and Griffiths, T.},\n",
    "  biburl = {https://www.bibsonomy.org/bibtex/2ab33268b60e8774bdfe46cd50a970fa3/folke},\n",
    "  chapter = {Probabilistic topic models},\n",
    "  editor = {Landauer, T. and McNamara, S. Dennis and Kintsch, W.},\n",
    "  keywords = {introduction lda lsa models topic},\n",
    "  owner = {gregor},\n",
    "  publisher = {Laurence Erlbaum},\n",
    "  title = {Latent Semantic Analysis: A Road to Meaning},\n",
    "  year = 2007\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "[5]\n",
    "@inproceedings{hofmann-1999-probabilistic-latent-semantic-analysis,\n",
    "  title={Probabilistic latent semantic analysis},\n",
    "  author={Hofmann, Thomas},\n",
    "  booktitle={Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence},\n",
    "  pages={289--296},\n",
    "  year={1999},\n",
    "  organization={Morgan Kaufmann Publishers Inc.}\n",
    "}\n",
    "\n",
    "[6]\n",
    "@article{Wang-2011-online-variational-inference-for-the-hierarchical-dirichlet-process,\n",
    "  author={Wang, C. and Paisley, J. and Blei, D.M.},\n",
    "  title={Online variational inference for the hierarchical Dirichlet process},\n",
    "  journal={Journal of Machine Learning Research},\n",
    "  year={2011},\n",
    "  volume={15},\n",
    "  pages={752-760},\n",
    "}\n",
    "\n",
    "[7]\n",
    "@Article{Blei-2003-latent-dirichlet-allocation,\n",
    "  author={Blei, D.M. and Ng, A.Y. and Jordan, M.I.},\n",
    "  title={Latent Dirichlet allocation},\n",
    "  journal={Journal of Machine Learning Research},\n",
    "  year={2003},\n",
    "  volume={3},\n",
    "  number={4-5},\n",
    "  pages={993-1022},\n",
    "}\n",
    "\n",
    "[8]\n",
    "@CONFERENCE{Röder-2015-exploring-the-space-of-topic-coherence-measures,\n",
    "  author={Röder, M. and Both, A. and Hinneburg, A.},\n",
    "  title={Exploring the space of topic coherence measures},\n",
    "  journal={WSDM 2015 - Proceedings of the 8th ACM International Conference on Web Search and Data Mining},\n",
    "  year={2015},\n",
    "  pages={399-408},\n",
    "  doi={10.1145/2684822.2685324},\n",
    "  publisher={Association for Computing Machinery, Inc},\n",
    "  document_type={Conference Paper},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
